{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drone Anomaly Detection Dataset\n",
    "\n",
    "AAI-530 Group 2 Final Project\n",
    "\n",
    "| Group Members |\n",
    "|:---|\n",
    "| Aliaksei Matsarski |\n",
    "| Andrew Fennimore |\n",
    "\n",
    "This notebook downloads the Drone Anomaly Detection Time Series Dataset from Kaggle, loads and inspects the data, runs some exploratory data analysis, and then splits it into train, validation, and test sets for modeling.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "This dataset contains preprocessed time series data for a binary classification task. The goal is to determine whether a drone is healthy or faulty based on its motion data. The data has already been windowed with no missing values.\n",
    "\n",
    "### Source\n",
    "\n",
    "The data is derived from the \"DronePropA: Motion Trajectories Dataset for Defective Drones\" by Ismail, Elshaar, et al. The original dataset consists of 130 .mat files, each representing a single flight experiment. You can find the processed version on [Kaggle](https://www.kaggle.com/datasets/PirMustafa/drone-dataset).\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The original .mat files were processed into a single .npz file. The following steps were applied (Information taken from the [Kaggle dataset page](https://www.kaggle.com/datasets/PirMustafa/drone-dataset)):\n",
    "\n",
    "1. Feature Extraction: For each of the 130 flights, 12 specific time series features were extracted focusing on the drone's core motion dynamics.\n",
    "2. Labeling: Each flight was labeled as healthy or faulty based on the file naming convention described in the source paper. Healthy is 0 and faulty is 1.\n",
    "3. Windowing: The time series data from each flight was segmented into overlapping windows. Each window is 200 timesteps long with a 50% overlap between consecutive windows.\n",
    "4. Aggregation: All windows from all flights were stacked into a single dataset.\n",
    "\n",
    "### Structure\n",
    "\n",
    "The data is contained in a single compressed NumPy archive file called processed_data.npz, which is around 637 MB. This file contains two arrays, X and y.\n",
    "\n",
    "X is a 3 dimensional NumPy array containing the feature data:\n",
    "- The first dimension is the total number of windows aggregated from all flights\n",
    "- The second dimension is the number of timesteps in each window, which is 200\n",
    "- The third dimension is the number of features recorded at each timestep, which is 12\n",
    "\n",
    "y is a 1 dimensional NumPy array containing the corresponding labels for each window in X. A value of 0 means the window came from a healthy flight, and a value of 1 means it came from a faulty flight.\n",
    "\n",
    "### Features\n",
    "\n",
    "The 12 features in the third dimension of X are, in order:\n",
    "\n",
    "1. Position X in meters\n",
    "2. Position Y in meters\n",
    "3. Position Z in meters\n",
    "4. Roll in radians\n",
    "5. Pitch in radians\n",
    "6. Yaw in radians\n",
    "7. Roll Rate in rad/s\n",
    "8. Pitch Rate in rad/s\n",
    "9. Yaw Rate in rad/s\n",
    "10. Acceleration X in m/s squared\n",
    "11. Acceleration Y in m/s squared\n",
    "12. Acceleration Z in m/s squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "Before running this notebook, make sure you have the following set up:\n",
    "\n",
    "1. A Kaggle account (you can sign up at [kaggle.com](https://www.kaggle.com))\n",
    "2. Install the required packages by running `pip install -r requirements.txt`\n",
    "3. The first time you run this, `kagglehub` will ask you to log in through your browser. After that it remembers your credentials so you won't need to do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the dataset from kaggle, it will cache locally so it only downloads once\n",
    "dataset_path = kagglehub.dataset_download(\"PirMustafa/drone-dataset\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect the Data\n",
    "\n",
    "Now that the dataset is downloaded, we can load the `.npz` file and take a look at what is inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# load up the npz file\nNPZ_FILE = os.path.join(dataset_path, \"processed_data.npz\")\ndata = np.load(NPZ_FILE)\n\n# see what arrays are in here\nprint(\"Arrays in the .npz file:\", list(data.keys()))\n\n# pull out features and labels\nX = data[\"X\"]\ny = data[\"y\"]\n\nprint(f\"\\nFeatures X shape: {X.shape}\")\nprint(f\"  - {X.shape[0]} windows\")\nprint(f\"  - {X.shape[1]} timesteps per window\")\nprint(f\"  - {X.shape[2]} features per timestep\")\n\nprint(f\"\\nLabels y shape: {y.shape}\")\nprint(f\"  - Unique labels: {np.unique(y)}\")\nprint(f\"  - Label distribution: 0 = healthy = {np.sum(y == 0)}, 1 = faulty = {np.sum(y == 1)}\")\n\nprint(f\"\\nX dtype: {X.dtype}\")\nprint(f\"y dtype: {y.dtype}\")\n\n# check for missing values\nnan_count = np.isnan(X).sum()\nprint(f\"\\nMissing values in X: {nan_count}\")\n\n# check for duplicate windows by flattening each window and comparing\nX_2d = X.reshape(X.shape[0], -1)\nn_duplicates = X_2d.shape[0] - np.unique(X_2d, axis = 0).shape[0]\nprint(f\"Duplicate windows: {n_duplicates}\")\n\n# check labels for anything unexpected\nprint(f\"\\nUnique labels in y: {np.unique(y)}\")\nprint(f\"Missing values in y: {np.isnan(y.astype(float)).sum()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Display First Windows as DataFrames\n",
    "\n",
    "Since `X` is a 3D array (windows x timesteps x features), each \"row\" is really a full window of 200 timesteps across 12 features. To get a better look at the actual values, we will convert the first two windows into pandas DataFrames with named columns and show the first and last few timesteps of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 12 feature names in order\n",
    "FEATURE_NAMES = [\n",
    "    \"Position_X\", \"Position_Y\", \"Position_Z\",\n",
    "    \"Roll\", \"Pitch\", \"Yaw\",\n",
    "    \"Roll_Rate\", \"Pitch_Rate\", \"Yaw_Rate\",\n",
    "    \"Acceleration_X\", \"Acceleration_Y\", \"Acceleration_Z\"\n",
    "]\n",
    "\n",
    "# show the first 2 windows as dataframes so we can eyeball the values\n",
    "for i in range(2):\n",
    "    window = X[i]  # 200 timesteps x 12 features\n",
    "    df = pd.DataFrame(window, columns = FEATURE_NAMES)\n",
    "    df.index.name = \"Timestep\"\n",
    "\n",
    "    label_str = \"Healthy\" if y[i] == 0 else \"Faulty\"\n",
    "\n",
    "    print(f\"\\n{\"=\" * 80}\")\n",
    "    print(f\"Window {i} | Label: {y[i]} - {label_str}\")\n",
    "    print(f\"{\"=\" * 80}\")\n",
    "\n",
    "    display(df.head(5))\n",
    "    print(f\"  ... {len(df) - 10} rows omitted ...\")\n",
    "    display(df.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Summary Statistics\n",
    "\n",
    "Here we run some quick sanity checks on the feature ranges and look at how the labels are distributed across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# flatten X down to 2D so we can run describe across all timesteps\nX_flat = X.reshape(-1, X.shape[2])\nstats_df = pd.DataFrame(X_flat, columns = FEATURE_NAMES).describe()\n\n# format the table so it doesnt show scientific notation\nprint(\"Overall feature statistics across all windows and timesteps:\")\ndisplay(stats_df.style.format(\"{:.4f}\"))\n\n# check the label balance\ntotal = len(y)\nhealthy = np.sum(y == 0)\nfaulty = np.sum(y == 1)\n\nprint(f\"\\nLabel balance:\")\nprint(f\"  Healthy: {healthy} - {100 * healthy / total:.1f}%\")\nprint(f\"  Faulty:  {faulty} - {100 * faulty / total:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "In this section we visualize the class balance, look at how the 12 features are distributed across healthy and faulty windows, and plot a sample window from each class to see what the raw time series data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart for class balance\n",
    "labels, counts = np.unique(y, return_counts = True)\n",
    "label_names = [\"Healthy\", \"Faulty\"]\n",
    "colors = [\"#3498db\", \"#e74c3c\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 5))\n",
    "bars = ax.bar(label_names, counts, color = colors, edgecolor = \"black\", width = 0.5)\n",
    "\n",
    "for bar, count in zip(bars, counts):\n",
    "    pct = 100 * count / len(y)\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() / 2,\n",
    "        f\"{count}\\n{pct:.1f}%\",\n",
    "        ha = \"center\", va = \"center\", fontweight = \"bold\", fontsize = 11, color = \"white\"\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Number of Windows\")\n",
    "ax.set_title(\"Class Balance: Healthy vs Faulty Windows\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature distributions using the per-window mean, split by class\n",
    "X_mean = X.mean(axis = 1)  # average each window down to 12 values\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize = (15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, name) in enumerate(zip(axes, FEATURE_NAMES)):\n",
    "    ax.hist(X_mean[y == 0, i], bins = 50, alpha = 0.6, color = \"#3498db\", label = \"Healthy\", density = True)\n",
    "    ax.hist(X_mean[y == 1, i], bins = 50, alpha = 0.6, color = \"#e74c3c\", label = \"Faulty\", density = True)\n",
    "    ax.set_title(name)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "fig.suptitle(\"Feature Distributions by Class\", fontsize = 14, y = 1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train / Validation / Test Split\n",
    "\n",
    "We split the data into 70% training, 15% validation, and 15% test sets using stratified sampling so the class balance stays consistent across all three sets. Since the dataset comes pre windowed and we do not have flight IDs, we cannot do a per flight split, so a stratified random split is the best we can do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first split: 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size = 0.30, random_state = 1, stratify = y\n",
    ")\n",
    "\n",
    "# second split: chop the 30% in half for 15% val and 15% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size = 0.50, random_state = 1, stratify = y_temp\n",
    ")\n",
    "\n",
    "# print out shapes and class balance for each split\n",
    "for name, X_split, y_split in [(\"Train\", X_train, y_train),\n",
    "                                (\"Validation\", X_val, y_val),\n",
    "                                (\"Test\", X_test, y_test)]:\n",
    "    n = len(y_split)\n",
    "    h = np.sum(y_split == 0)\n",
    "    f = np.sum(y_split == 1)\n",
    "\n",
    "    print(f\"{name:>10s}: X = {X_split.shape}, y = {y_split.shape}  |  \"\n",
    "          f\"Healthy = {h} - {100 * h / n:.1f}%  Faulty = {f} - {100 * f / n:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Total: {len(y_train)} + {len(y_val)} + {len(y_test)} = {len(y_train) + len(y_val) + len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Splits\n",
    "\n",
    "We save each split as a `.npz` file in the `data/` directory so they are easy to load later during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each split to the data folder as npz files\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")\n",
    "os.makedirs(DATA_DIR, exist_ok = True)\n",
    "\n",
    "np.savez(os.path.join(DATA_DIR, \"train.npz\"), X = X_train, y = y_train)\n",
    "np.savez(os.path.join(DATA_DIR, \"validation.npz\"), X = X_val, y = y_val)\n",
    "np.savez(os.path.join(DATA_DIR, \"test.npz\"), X = X_test, y = y_test)\n",
    "\n",
    "print(\"Saved to data/:\")\n",
    "\n",
    "for name in [\"train.npz\", \"validation.npz\", \"test.npz\"]:\n",
    "    d = np.load(os.path.join(DATA_DIR, name))\n",
    "    print(f\"  {name}: X = {d['X'].shape}, y = {d['y'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LSTM Model\n",
    "\n",
    "Now we can build and train an LSTM to classify each window as healthy or faulty. The model takes in the full sequence of 200 timesteps across all 12 features and learns to pick up on patterns that separate the two classes. We wrap everything in a class so its easy to train and make predictions with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, num_units, n_classes, n_input,\n",
    "                 time_steps, learning_rate = 0.001):\n",
    "        self.steps = time_steps\n",
    "        self.n = n_input\n",
    "\n",
    "        # build the lstm model\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.LSTM(num_units, input_shape = (time_steps, n_input)),\n",
    "            tf.keras.layers.Dense(n_classes, activation = \"softmax\")\n",
    "        ])\n",
    "\n",
    "        # compile with adam optimizer and cross entropy loss\n",
    "        self.model.compile(\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "            loss = \"sparse_categorical_crossentropy\",\n",
    "            metrics = [\"accuracy\"]\n",
    "        )\n",
    "\n",
    "    def train(self, X, Y, epochs = 100, batch_size = 128):\n",
    "        # reshape input to match what the lstm expects\n",
    "        X = X.reshape((len(X), self.steps, self.n))\n",
    "\n",
    "        # train the model\n",
    "        self.history = self.model.fit(X, Y, epochs = epochs, batch_size = batch_size)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # predicting the output\n",
    "        test_data = X.reshape((-1, self.steps, self.n))\n",
    "        out = self.model.predict(test_data)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# load the split data from the saved npz files\nDATA_DIR = os.path.join(os.getcwd(), \"data\")\ntrain_data = np.load(os.path.join(DATA_DIR, \"train.npz\"))\nval_data = np.load(os.path.join(DATA_DIR, \"validation.npz\"))\ntest_data = np.load(os.path.join(DATA_DIR, \"test.npz\"))\n\nX_train, y_train = train_data[\"X\"], train_data[\"y\"]\nX_val, y_val = val_data[\"X\"], val_data[\"y\"]\nX_test, y_test = test_data[\"X\"], test_data[\"y\"]\n\n# create the lstm model\n# 128 hidden units, 2 classes, 12 features, 200 timesteps\nmodel = LSTM(\n    num_units = 128,\n    n_classes = 2,\n    n_input = 12,\n    time_steps = 200,\n    learning_rate = 0.001\n)\n\n# train on the training data\nmodel.train(X_train, y_train, epochs = 50, batch_size = 128)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis = 1)\n",
    "\n",
    "# check how we did\n",
    "test_accuracy = np.mean(predicted_labels == y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predicted_labels))\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predicted_labels, target_names = [\"Healthy\", \"Faulty\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "At this point the dataset has been downloaded, inspected, explored, and split into train, validation, and test sets. From here we can move on to:\n",
    "\n",
    "- Model selection and training (LSTM, GRU, 1D CNN, etc.)\n",
    "- Hyperparameter tuning\n",
    "- Evaluation and comparison of results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}